{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN7RFLsdjFXe8Pbval4bIYt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JimCurryWang/Deep-Learning-Jot/blob/seq2seq-1/Transformer/2_Learning_Phrase_Representations_using_RNN_Encoder_Decoder_for_Statistical_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zec6kJAvY990"
      },
      "source": [
        "# 2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
        "\n",
        "\n",
        "- Reference: [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
        "\n",
        "-  Improving test perplexity while only using a single layer RNN in both the encoder and the decoder compared to first model in [1 - Sequence to Sequence Learning with Neural Networks](https://github.com/JimCurryWang/Deep-Learning-Jot/blob/seq2seq-1/Transformer/1_Sequence_to_Sequence_Learning_with_Neural_Networks.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1FGqD8Kv-rD3pbDpOTVT9I-WIUeLqAYum?usp=sharing)\n",
        "\n",
        "\n",
        "# Introduction\n",
        "- Learning Points\n",
        "  - Re-use context vector (z) in decoder part for rnn component and final prediction linear layer output.\n",
        "  - How to concatenate embedded and context together?\n",
        "  - Why concatenate work in here? (a kind of Shortcut?)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j32Adhv7Y-zG"
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E34J_mKyaA92"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Z4w3Nba-Ha"
      },
      "source": [
        "# Load Model & Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuC0MPGtaC0k"
      },
      "source": [
        "# Load Spacy model\n",
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "# tokenizer functions\n",
        "# transformed sentence into a list of tokens\n",
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
        "    (SRC-source)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings (tokens)\n",
        "    (TRG-target)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "\n",
        "# torchtext's Field\n",
        "SRC = Field(tokenize = tokenize_de, \n",
        "          init_token = '<sos>', \n",
        "          eos_token = '<eos>', \n",
        "          lower = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "# Download Multi30k dataset\n",
        "train_data, valid_data, test_data = Multi30k.splits(\n",
        "    exts = ('.de', '.en'), fields = (SRC, TRG))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFfjRZeZaqJa",
        "outputId": "c9dc87d6-ba9b-427a-f663-447bb3cc48bf"
      },
      "source": [
        "# check the data by its length\n",
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
        "\n",
        "# print out an single example, make sure the source sentence is reversed:\n",
        "pprint(vars(train_data.examples[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n",
            "{'src': ['.',\n",
            "         'büsche',\n",
            "         'vieler',\n",
            "         'nähe',\n",
            "         'der',\n",
            "         'in',\n",
            "         'freien',\n",
            "         'im',\n",
            "         'sind',\n",
            "         'männer',\n",
            "         'weiße',\n",
            "         'junge',\n",
            "         'zwei'],\n",
            " 'trg': ['two',\n",
            "         'young',\n",
            "         ',',\n",
            "         'white',\n",
            "         'males',\n",
            "         'are',\n",
            "         'outside',\n",
            "         'near',\n",
            "         'many',\n",
            "         'bushes',\n",
            "         '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs0L3mrCa5mB"
      },
      "source": [
        "# Build Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovHoYqGtaofR",
        "outputId": "476fc46f-6f9a-44a2-bd3e-47bc7dda3d93"
      },
      "source": [
        "# use Filed object to build vocabulary\n",
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)\n",
        "\n",
        "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (de) vocabulary: 7855\n",
            "Unique tokens in target (en) vocabulary: 5893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OJbVyi_a17O"
      },
      "source": [
        "# Build iterators (BucketIterator*) for DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2R85pX7aYJ-"
      },
      "source": [
        "# Build iterators (BucketIterator*) for DataSet\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIqVqm8dbNd2"
      },
      "source": [
        "# Building the Seq2Seq Model\n",
        "\n",
        "## Encoder \n",
        "\n",
        "- Single-layer GRU\n",
        "\n",
        "- The encoder is similar to the previous one, with the multi-layer LSTM swapped for a single-layer GRU. \n",
        "\n",
        "- As we only have a single layer, we also don't pass the dropout as an argument to the GRU as that dropout is used between each layer of a multi-layered RNN. \n",
        "\n",
        "## Decoder\n",
        "\n",
        "- The decoder is where the implementation differs significantly from the previous model and we alleviate some of the information compression.\n",
        "\n",
        "- re-use context vector (z) in rnn component\n",
        "  - re-use the same context vector returned by the encoder for every time-step in the decoder.\n",
        "  \n",
        "  - $$s_t = \\text{DecoderGRU}(d(y_t), s_{t-1}, z)$$\n",
        "\n",
        "- re-use contect vector in linear layer\n",
        "  - Before, we predicted the next token, $\\hat{y}_{t+1}$, with the linear layer, $f$, only using the top-layer decoder hidden state at that time-step, $s_t$, as $\\hat{y}_{t+1}=f(s_t^L)$. \n",
        "  - Now, we also pass the embedding of current token, $d(y_t)$ and the context vector, $z$ to the linear layer.\n",
        "\n",
        "  - $$\\hat{y}_{t+1} = f(d(y_t), s_t, z)$$\n",
        "\n",
        "-  Decoding single token per time-step (same as model-1)\n",
        "\n",
        "  - The Decoder class does a single step of decoding. i.e. it ouputs single token per time-step. only decoding one token at a time, the input tokens will always have a sequence length of 1\n",
        "\n",
        "-  Context vectors as fist input in Decoder (same as model-1)\n",
        "\n",
        "  - the initial hidden and cell states to our decoder are our context vectors, which are the final hidden and cell states of our encoder from the same layer, i.e. $(s_0^l,c_0^l)=z^l=(h_T^l,c_T^l)$.\n",
        "\n",
        "  - Note, the initial hidden state, $s_0$, is still the context vector, $z$, so when generating the first token we are actually inputting two identical context vectors into the GRU.\n",
        "\n",
        "\n",
        "## Rethinking (Irresponsible Suggestion)\n",
        "- The idea of re-using the context feature is somehow like adding short cut in U-net and ResNet?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8Z1y1tebFa6"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        '''\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        # no dropout as only one layer!\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=input_dim, embedding_dim=emb_dim\n",
        "        )\n",
        "        \n",
        "        self.rnn = nn.GRU(input_size=emb_dim, hidden_size=hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        '''\n",
        "        # src = [src len, batch size]\n",
        "        # embedded = [src len, batch size, emb dim]\n",
        "        # outputs = [src len, batch size, hid dim * n directions]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # outputs are always from the top hidden layer  \n",
        "        '''\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "              \n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=output_dim, embedding_dim=emb_dim\n",
        "        )\n",
        "        \n",
        "        self.rnn = nn.GRU(\n",
        "            input_size= emb_dim + hid_dim, \n",
        "            hidden_size=hid_dim\n",
        "        )\n",
        "        \n",
        "        self.fc_out = nn.Linear(\n",
        "            in_features = emb_dim + hid_dim * 2, \n",
        "            out_features = output_dim\n",
        "        )\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, context):\n",
        "        '''\n",
        "        # input = [batch size]\n",
        "        # hidden = [n layers * n directions, batch size, hid dim]\n",
        "        # context = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        # n layers and n directions in the decoder will both always be 1, therefore:\n",
        "        # hidden = [1, batch size, hid dim]\n",
        "        # context = [1, batch size, hid dim]\n",
        "        '''\n",
        "        # Decoding single token per time-step, \n",
        "        # so the input tokens will always have a sequence length of 1\n",
        "\n",
        "        input = input.unsqueeze(0) \n",
        "        # -> input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input)) \n",
        "        # -> embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\n",
        "        # -> emb_con = [1, batch size, emb dim + hid dim]\n",
        "            \n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "        \n",
        "        # -> output = [seq len, batch size, hid dim * n directions]\n",
        "        # -> hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
        "        # -> output = [1, batch size, hid dim]\n",
        "        # -> hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        output = torch.cat(\n",
        "            (embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \n",
        "            dim = 1\n",
        "        )\n",
        "        # -> output = [batch size, emb dim + hid dim + contect dim]      \n",
        "        #           = [batch size, emb dim + hid dim * 2]\n",
        "        \n",
        "        prediction = self.fc_out(output)\n",
        "        # -> prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y1i4BYe_X4Q"
      },
      "source": [
        "# Seq2Seq\n",
        "\n",
        "- the `outputs` tensor is created to hold all predictions, $\\hat{Y}$\n",
        "- the source sequence, $X$, is fed into the encoder to receive a `context` vector\n",
        "- the initial decoder hidden state is set to be the `context` vector, $s_0 = z = h_T$\n",
        "- we use a batch of `<sos>` tokens as the first `input`, $y_1$\n",
        "- we then decode within a loop:\n",
        "  - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and the context vector, $z$, into the decoder\n",
        "  - receiving a prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\n",
        "  - we then decide if we are going to teacher force or not, setting the next input as appropriate (either the ground truth next token in the target sequence or the highest predicted next token)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIF2bweV6BIO"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        '''\n",
        "        # src = [src len, batch size]\n",
        "        # trg = [trg len, batch size]\n",
        "        \n",
        "        # teacher_forcing_ratio is probability to use teacher forcing\n",
        "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        '''\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # ---- Encoder ----\n",
        "        # last hidden state of the encoder is the context\n",
        "        context = self.encoder(src)\n",
        "        \n",
        "        # ---- Decoder ----\n",
        "        # context also used as the initial hidden state of the decoder\n",
        "        hidden = context\n",
        "        \n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        # [<sos>, y1, y2, y3 ]\n",
        "        # trg = [seq len, batch size], trg[0,:] -> first words for whole the batch\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            # insert input token embedding, previous hidden state and the context state\n",
        "            # receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "            \n",
        "            # place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            # decide if we are going to use teacher forcing or not\n",
        "            # teacher_force = [True, False]\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            # if teacher forcing, use actual next token as next input\n",
        "            # if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slgV7sGbCHvL"
      },
      "source": [
        "# Training \n",
        "\n",
        "- Initialize our parameters. \n",
        "  - The paper states the parameters are initialized from a normal distribution with a mean of 0 and a standard deviation of 0.01, i.e.  N(0,0.01).\n",
        "\n",
        "  - It also states we should initialize the recurrent parameters to a special initialization, however to keep things simple we'll also initialize them to  N(0,0.01).\n",
        "\n",
        "\n",
        "- Optimizer & Loss Function\n",
        "  - initiaize our optimizer.\n",
        "  - initialize the loss function, making sure to ignore the loss on <pad> tokens.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aJI-ZEMBram",
        "outputId": "d90ab3a1-035e-49c1-cb4a-444730960175"
      },
      "source": [
        "'''Note\n",
        "# i.g. \n",
        "# TRG_PAD_IDX = 1 \n",
        "# TRG.pad_token = '<pad>'\n",
        "'''\n",
        "\n",
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "\n",
        "def init_weights_uniform(m):\n",
        "    '''initialize weights \n",
        "    with a uniform distribution(nn.init.uniform_) between -0.08 and +0.08\n",
        "    '''\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "def init_weights_normal(m):\n",
        "    '''initialize weights \n",
        "    with a normal distribution(nn.init.normal_) with a mean of 0 \n",
        "    and a standard deviation of 0.01, \n",
        "    i.e. N(0,0.01) .\n",
        "    '''\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "\n",
        "def count_parameters(model):\n",
        "    '''calculate the number of trainable parameters in the model\n",
        "    '''\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# model\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "model.apply(init_weights_normal)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnBbVMA8HTyY"
      },
      "source": [
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "# loss function\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkBn6-JxKQnd",
        "outputId": "d0889346-3e8f-402c-f26b-06ded504c25f"
      },
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 14,220,293 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzzcU1vELEMG"
      },
      "source": [
        "# Define Train & evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4cRc8K-LFxz"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        # --- forward --- \n",
        "        src = batch.src\n",
        "        trg = batch.trg \n",
        "        output = model(src, trg)\n",
        "        # -> trg = [trg len, batch size]\n",
        "        # -> output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        # -> output = [(trg len - 1) * batch size, output dim]\n",
        "        # -> trg = [(trg len - 1) * batch size]\n",
        "        \n",
        "        # loss computation\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        # --- backward --- \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # gradient descent update step/adam step\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # gradient clipping\n",
        "        optimizer.step()\n",
        "        \n",
        "        # loss computation records\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            # --- forward ---\n",
        "            # turn off teacher forcing\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            output = model(src, trg, 0)\n",
        "\n",
        "            # -> trg = [trg len, batch size]\n",
        "            # -> output = [trg len, batch size, output dim]\n",
        "\n",
        "            # loss function\n",
        "            # Allign the target and predicted_out\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "            \n",
        "            # -> output = [(trg len - 1) * batch size, output dim]\n",
        "            # -> trg = [(trg len - 1) * batch size]\n",
        "          \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # loss computation records\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8Y_4NiRPKVv"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF9pfi2OPdRO"
      },
      "source": [
        "# Load Model & Evaluate on Test Set\n",
        "\n",
        "### Relieving the information compression looks good!\n",
        "- Just looking at the test loss, we get better performance than the previous model. \n",
        "- This is a pretty good sign that this model architecture is doing something right! \n",
        "- Relieving the information compression seems like the way forard.\n",
        "\n",
        "### What can be improved next?\n",
        "- Attention is all you need\n",
        "  - In the next chapter we'll expand on this even further with `attention`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiRPMplZPXoZ"
      },
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}