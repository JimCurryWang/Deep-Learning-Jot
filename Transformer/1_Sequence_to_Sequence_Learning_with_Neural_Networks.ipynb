{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - Sequence to Sequence Learning with Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPun3oCLYJ6H2nD35Kjul+v"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQVzwHge8O-5"
      },
      "source": [
        "# 1 - Sequence to Sequence Learning with Neural Networks\n",
        "\n",
        "+ In this series we'll be building a machine learning model to go from once sequence to another, using PyTorch and torchtext.\n",
        "+ This will be done on German to English translations with [Multi30k dataset*](https://github.com/multi30k/dataset).\n",
        "\n",
        "+ spaCy to assist in the tokenization of the data\n",
        "\n",
        "  ```python\n",
        "  python -m spacy download en_core_web_sm\n",
        "  python -m spacy download de_core_news_sm\n",
        "  ```\n",
        "\n",
        "+ Reference: [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) paper\n",
        "\n",
        "+ *Multi30k is a dataset with ~30,000 parallel English, German and French sentences, each with ~12 words per sentence. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDc7lWES8i7N"
      },
      "source": [
        "# Introduction\n",
        "+ The hidden state $(h_{i})$ as a vector representation of the sentence\n",
        "+ The context vector $(z)$ as an abstract representation of the entire input sentence\n",
        "\n",
        "+ **Decoder**, one per time-step.\n",
        "\n",
        "  In the decoder, we need to go from the hidden state to an actual word, therefore at each time-step we use $s_t$ to predict (by passing it through a `Linear` layer, shown in purple) what we think is the next word in the sequence, $\\hat{y}_t$. \n",
        "\n",
        "  $$\\hat{y}_t = f(s_t)$$\n",
        "\n",
        "  The words in the decoder are always generated one after another, with one per time-step. \n",
        "\n",
        "+ **Teacher Forcing**: ground truth + predicted word    \n",
        "    \n",
        "  We always use `<sos>` for the first input to the decoder, $y_1$, but for subsequent inputs, $y_{t>1}$, we will sometimes use the actual, ground truth next word in the sequence, $y_t$ and sometimes use the word predicted by our decoder, $\\hat{y}_{t-1}$. This is called *teacher forcing*, see a bit more info about it [Teacher Forcing](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/). \n",
        "\n",
        "+ **Know the length in advance**\n",
        "\n",
        "  When training/testing our model, we always know how many words are in our target sentence, so we stop generating words once we hit that many. During inference it is common to keep generating words until the model outputs an `<eos>` token or after a certain amount of words have been generated.\n",
        "\n",
        "+ **Calculate the loss**\n",
        "\n",
        "  Once we have our predicted target sentence, $\\hat{Y} = \\{ \\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_T \\}$, we compare it against our actual target sentence, $Y = \\{ y_{1}, y_{2}, ..., y_{T} \\}$, to calculate our loss. We then use this loss to update all of the parameters in our model.\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ecu4gDPuLtbh"
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nBks1k4HHDr"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BrGyxgjJz8s"
      },
      "source": [
        "+ Load Spacy model\n",
        "+ Tokenizer Functions\n",
        "  + reverse the order of the input\n",
        "  + normal order for the output\n",
        "\n",
        "  In the paper we are implementing, they find it beneficial to reverse the order of the input which they believe \"introduces many short term dependencies in the data that make the optimization problem much easier\". We copy this by reversing the German sentence after it has been transformed into a list of tokens.\n",
        "\n",
        "\n",
        "+ torchtext's Fields \n",
        "  \n",
        "  torchtext's Fields handle how data should be processed.\n",
        "\n",
        "  + appends the \"start of sequence\" and \"end of sequence\" tokens\n",
        "  + converts all words to lowercase\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbmgzfQzIrZL",
        "outputId": "fb6b33f8-adf8-4874-a723-e9c378136ad6"
      },
      "source": [
        "# Load Spacy model\n",
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "# tokenizer functions\n",
        "# transformed sentence into a list of tokens\n",
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
        "    (SRC-source)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings (tokens)\n",
        "    (TRG-target)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "\n",
        "# torchtext's Field\n",
        "SRC = Field(tokenize = tokenize_de, \n",
        "          init_token = '<sos>', \n",
        "          eos_token = '<eos>', \n",
        "          lower = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "# Download Multi30k dataset\n",
        "train_data, valid_data, test_data = Multi30k.splits(\n",
        "    exts = ('.de', '.en'), fields = (SRC, TRG))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rtraining.tar.gz:   0%|          | 0.00/1.21M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00<00:00, 8.04MB/s]\n",
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 1.64MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n",
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 1.33MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trRZpUAcLk-X",
        "outputId": "ea3c4005-76fa-4588-b17c-9fa70c0c3f1a"
      },
      "source": [
        "# check the data by its length\n",
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
        "\n",
        "# print out an single example, make sure the source sentence is reversed:\n",
        "from pprint import pprint\n",
        "pprint(vars(train_data.examples[0]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n",
            "{'src': ['.',\n",
            "         'büsche',\n",
            "         'vieler',\n",
            "         'nähe',\n",
            "         'der',\n",
            "         'in',\n",
            "         'freien',\n",
            "         'im',\n",
            "         'sind',\n",
            "         'männer',\n",
            "         'weiße',\n",
            "         'junge',\n",
            "         'zwei'],\n",
            " 'trg': ['two',\n",
            "         'young',\n",
            "         ',',\n",
            "         'white',\n",
            "         'males',\n",
            "         'are',\n",
            "         'outside',\n",
            "         'near',\n",
            "         'many',\n",
            "         'bushes',\n",
            "         '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "necgzB9HTO1j"
      },
      "source": [
        "# Build Vocabulary\n",
        "use Torchtext's Filed object to build vocabulary\n",
        "\n",
        "+ Build the vocabulary for the source and target languages. \n",
        "\n",
        "  The vocabulary is used to associate each unique token with an index (an integer). The vocabularies of the source and target languages are distinct.\n",
        "\n",
        "+ Frequency Condition and Unknow Token(`<unk>`)\n",
        "\n",
        "  Using the `min_freq` argument, we only allow tokens that appear at least 2 times to appear in our vocabulary. Tokens that appear only once are converted into an `<unk>` (unknown) token.\n",
        "\n",
        "+ Vocabulary only be built from training set\n",
        "\n",
        "  It is important to note that our vocabulary should only be built from the training set and not the validation/test set. This prevents \"information leakage\" into our model, giving us artifically inflated validation/test scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fugNACQzNPbD",
        "outputId": "17746fde-d971-400b-f34a-b16ac17d2b52"
      },
      "source": [
        "# use Filed object to build vocabulary\n",
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)\n",
        "\n",
        "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (de) vocabulary: 7855\n",
            "Unique tokens in target (en) vocabulary: 5893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bOzfEAKWsIA"
      },
      "source": [
        "# Build iterators (BucketIterator*) for DataSet \n",
        "\n",
        "#### BucketIterator\n",
        "\n",
        "In NLP, when we get a batch of examples using an iterator we need to make sure that all of the source sentences are padded to the same length, the same with the target sentences.\n",
        "\n",
        "We use a `BucketIterator` instead of the standard `Iterator` as it creates batches in such a way that it minimizes the amount of padding in both the source and target sentences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-uoHZE3WWRP"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIkkBdPAXpE0"
      },
      "source": [
        "# Building the Seq2Seq Model\n",
        "\n",
        "## Encoder \n",
        "\n",
        "+ 2 layer LSTM \n",
        "\n",
        "  (The paper we are implementing uses a 4-layer LSTM, but in the interest of training time we cut this down to 2-layers.)\n",
        "\n",
        "## Decoder\n",
        "\n",
        "+ Also be a 2-layer (4 in the paper) LSTM\n",
        "\n",
        "+ Decoding single token per time-step\n",
        "\n",
        "  The Decoder class does a single step of decoding. i.e. it ouputs single token per time-step. only decoding one token at a time, the input tokens will always have a sequence length of 1\n",
        "\n",
        "+ Context vectors as fist input in Decoder\n",
        "\n",
        "  the initial hidden and cell states to our decoder are our context vectors, which are the final hidden and cell states of our encoder from the same layer, i.e. $(s_0^l,c_0^l)=z^l=(h_T^l,c_T^l)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5wTzSEIXlqT"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=input_dim, embedding_dim=emb_dim\n",
        "        )\n",
        "        \n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=emb_dim, hidden_size=hid_dim, \n",
        "            num_layers=n_layers, dropout=dropout\n",
        "        )\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        '''\n",
        "        #src = [src len, batch size]\n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # e.g.\n",
        "        # hidden = torch.Size([2, 128, 512])\n",
        "        #   cell = torch.Size([2, 128, 512])\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        '''        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "              \n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=output_dim, embedding_dim=emb_dim\n",
        "        )\n",
        "        \n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=emb_dim, hidden_size=hid_dim, \n",
        "            num_layers=n_layers, dropout=dropout\n",
        "        )\n",
        "        \n",
        "        self.fc_out = nn.Linear(\n",
        "            in_features=hid_dim, out_features=output_dim\n",
        "        )\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        '''\n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #In our case, n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "\n",
        "\n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        '''\n",
        "        # Decoding single token per time-step, \n",
        "        # so the input tokens will always have a sequence length of 1\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "                \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYKB9NRXXAAw"
      },
      "source": [
        "## Seq2Seq\n",
        "\n",
        "+ receiving the input/source sentence\n",
        "+ using the encoder to produce the context vectors\n",
        "+ using the decoder to produce the predicted output/target sentence\n",
        "\n",
        "\n",
        "#### Forward Step\n",
        "\n",
        "+ The forward method takes \n",
        "  + the source sentence\n",
        "  + the target sentence\n",
        "  + a teacher-forcing ratio, used when training our model.\n",
        "\n",
        "+ Output storage\n",
        "  + Create an outputs tensor that will store all of our predictions, $\\hat{y}$.\n",
        "\n",
        "#### Teacher Force\n",
        "\n",
        "+ When decoding, at each time-step we will predict what the next token in the target sequence will be from the previous tokens decoded. \n",
        "\n",
        "+ With probability equal to the `teacher_forcing_ratio` we will use the actual ground-truth next token in the sequence as the input to the decoder during the next time-step. \n",
        "\n",
        "+ However, with probability (`1 - teacher_forcing_ratio`), we will use the token that the model predicted as the next input to the model, even if it doesn't match the actual next token in the sequence.\n",
        "\n",
        "+ Note:\n",
        "  + $$ R = teacher\\_forcing\\_ratio $$ \n",
        "\n",
        "  $$ f(p)= \\begin{cases} R, & \\text {use ground-truth} \\\\ 1 - R, & \\text{use predicted from argmax} \\end{cases} $$\n",
        "\n",
        "  + if $P < R$, the next `input` is the ground-truth next token in the sequence, $y_{t+1}$\n",
        "\n",
        "  + if $P >= R$, the next `input` is the predicted next token in the sequence, $\\hat{y}_{t+1}$, which we get by doing an `argmax` over the output tensor\n",
        "  \n",
        "  + e.g. if teacher_forcing_ratio is 0.75 we use ground-truth as encoder inputs 75% of the time\n",
        "\n",
        "\n",
        "#### Decoder Loop \n",
        "\n",
        "+ The decoder loop starts at 1, not 0. \n",
        "+ This means the 0th element of our `outputs` tensor remains all zeros. So our `target` and `outputs` will look something like:\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\text{target} = [<sos>, &y_1, y_2, y_3, <eos>]\\\\\n",
        "\\text{outputs} = [0, &\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "+ Later on when we calculate the loss, we cut off the first element of each tensor to get:\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\text{target} = [&y_1, y_2, y_3, <eos>]\\\\\n",
        "\\text{outputs} = [&\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n",
        "\\end{align*}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t_xmFBgVtqY"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        # This is not always the case to let both dim and layers be equal.\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        '''\n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        \n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth as encoder inputs 75% of the time\n",
        "        \n",
        "        '''\n",
        "      \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        # ---- Encoder ----\n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        # ---- Decoder ----\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        # [<sos>, y1, y2, y3 ]\n",
        "        # trg = [seq len, batch size], trg[0,:] -> first words for whole the batch\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        #loop will start from 1 \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            #get the highest predicted token from our predictions\n",
        "            predicted_top1 = output.argmax(1) \n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            #teacher_force = [True, False]\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else predicted_top1\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1LHprmIqQDS"
      },
      "source": [
        "# Training\n",
        "\n",
        "## Optimizer & Loss Function\n",
        "\n",
        "+  loss function calculates the average loss per token.\n",
        "+  by passing the index of the `<pad>` token as the ignore_index argument, we ignore the loss whenever the target token is a padding token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbJwrIH5loKf",
        "outputId": "27d533fa-5a20-42bb-842d-f006647b75ce"
      },
      "source": [
        "'''Note\n",
        "# i.g. \n",
        "# TRG_PAD_IDX = 1 \n",
        "# TRG.pad_token = '<pad>'\n",
        "'''\n",
        "\n",
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    '''initialize weights \n",
        "    with a uniform distribution(nn.init.uniform_) between -0.08 and +0.08\n",
        "    '''\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "def count_parameters(model):\n",
        "    '''calculate the number of trainable parameters in the model\n",
        "    '''\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# model\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7MhRA0cOT8n"
      },
      "source": [
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "# loss function\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba14NALvlpfF",
        "outputId": "d1dd4095-10fd-4488-8e84-10063d87de6a"
      },
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 13,899,013 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "701ux6R4MAIP"
      },
      "source": [
        "# ------ testing block ------\n",
        "\n",
        "# for i, batch in enumerate(train_iterator):\n",
        "#   # print(i,batch)\n",
        "#   src = batch.src\n",
        "#   trg = batch.trg\n",
        "\n",
        "#   # print(src.shape)\n",
        "#   # print(trg.shape)\n",
        "\n",
        "#   # print(trg[:,0])\n",
        "#   # print(trg[0,:])\n",
        "\n",
        "#   output = model(src, trg)\n",
        "#   break\n",
        "\n",
        "\n",
        "# # for i in range(1,4):\n",
        "# #   print(i)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cSME-Odv3GJ"
      },
      "source": [
        "# Define Train & evaluate function\n",
        "\n",
        "#### Loss and Perplexity\n",
        "+ We'll be printing out both the loss and the perplexity at each epoch. \n",
        "+ It is easier to see a change in perplexity than a change in loss as the numbers are much bigger.\n",
        "\n",
        "$$Perplexity = math.exp(Loss)$$\n",
        "\n",
        "\n",
        "#### $math.exp()$  vs  $numpy.exp()$\n",
        "+ Exponential function.\n",
        "\n",
        "+ The `math.exp` works only for scalars, whereas `numpy.exp` will work for arrays.\n",
        "\n",
        "  ```python\n",
        "  >>> import math\n",
        "  >>> import numpy as np\n",
        "  >>> x = [1.,2.,3.,4.,5.]\n",
        "  >>> math.exp(x)\n",
        "\n",
        "  Traceback (most recent call last):\n",
        "    File \"<pyshell#10>\", line 1, in <module>\n",
        "      math.exp(x)\n",
        "  TypeError: a float is required\n",
        "  >>> np.exp(x)\n",
        "  array([   2.71828183,    7.3890561 ,   20.08553692,   54.59815003,\n",
        "          148.4131591 ])\n",
        "  ```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2kl4GdrvOSd"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg #trg = [trg len, batch size]\n",
        "        \n",
        "\n",
        "        # forward\n",
        "        output = model(src, trg) #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        # loss function\n",
        "        # Allign the target and predicted_out\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim) #output = [(trg len - 1) * batch size, output dim]\n",
        "        trg = trg[1:].view(-1) #trg = [(trg len - 1) * batch size]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # gradient descent update step/adam step\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # gradient clipping\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss computation records\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg #trg = [trg len, batch size]\n",
        "\n",
        "            #forward\n",
        "            #turn off teacher forcing\n",
        "            output = model(src, trg, 0) #output = [trg len, batch size, output dim]\n",
        "\n",
        "            # loss function\n",
        "            # Allign the target and predicted_out\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim) #output = [(trg len - 1) * batch size, output dim]\n",
        "            trg = trg[1:].view(-1) #trg = [(trg len - 1) * batch size]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            # loss computation records\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsaMIes2wmqX",
        "outputId": "3adc1ba1-7e29-467e-b75d-4ba9c31e36df"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 24s\n",
            "\tTrain Loss: 5.038 | Train PPL: 154.161\n",
            "\t Val. Loss: 5.057 |  Val. PPL: 157.163\n",
            "Epoch: 02 | Time: 0m 24s\n",
            "\tTrain Loss: 4.502 | Train PPL:  90.154\n",
            "\t Val. Loss: 4.804 |  Val. PPL: 122.017\n",
            "Epoch: 03 | Time: 0m 24s\n",
            "\tTrain Loss: 4.172 | Train PPL:  64.823\n",
            "\t Val. Loss: 4.648 |  Val. PPL: 104.347\n",
            "Epoch: 04 | Time: 0m 24s\n",
            "\tTrain Loss: 3.936 | Train PPL:  51.203\n",
            "\t Val. Loss: 4.406 |  Val. PPL:  81.964\n",
            "Epoch: 05 | Time: 0m 24s\n",
            "\tTrain Loss: 3.746 | Train PPL:  42.359\n",
            "\t Val. Loss: 4.222 |  Val. PPL:  68.164\n",
            "Epoch: 06 | Time: 0m 24s\n",
            "\tTrain Loss: 3.597 | Train PPL:  36.495\n",
            "\t Val. Loss: 4.220 |  Val. PPL:  68.057\n",
            "Epoch: 07 | Time: 0m 24s\n",
            "\tTrain Loss: 3.473 | Train PPL:  32.234\n",
            "\t Val. Loss: 4.053 |  Val. PPL:  57.585\n",
            "Epoch: 08 | Time: 0m 24s\n",
            "\tTrain Loss: 3.317 | Train PPL:  27.564\n",
            "\t Val. Loss: 3.972 |  Val. PPL:  53.065\n",
            "Epoch: 09 | Time: 0m 24s\n",
            "\tTrain Loss: 3.195 | Train PPL:  24.408\n",
            "\t Val. Loss: 3.967 |  Val. PPL:  52.800\n",
            "Epoch: 10 | Time: 0m 24s\n",
            "\tTrain Loss: 3.082 | Train PPL:  21.810\n",
            "\t Val. Loss: 3.840 |  Val. PPL:  46.520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FNQ26d-xEt7"
      },
      "source": [
        "# Load Model & Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFD51evlwmJz",
        "outputId": "6219063e-1364-47c9-8b91-8fc63e112c1d"
      },
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.815 | Test PPL:  45.389 |\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}