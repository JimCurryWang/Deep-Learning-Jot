{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 - Sequence to Sequence Learning with Neural Networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNhIXPXGh0bhP6NU9IDVdUB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JimCurryWang/Deep-Learning-Jot/blob/seq2seq-1/Transformer/1_Sequence_to_Sequence_Learning_with_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQVzwHge8O-5"
      },
      "source": [
        "# 1 - Sequence to Sequence Learning with Neural Networks\n",
        "\n",
        "+ In this series we'll be building a machine learning model to go from once sequence to another, using PyTorch and torchtext.\n",
        "+ This will be done on German to English translations. \n",
        "\n",
        "+ spaCy to assist in the tokenization of the data\n",
        "\n",
        "+ Reference: [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDc7lWES8i7N"
      },
      "source": [
        "# Introduction\n",
        "+ The hidden state $(h_{i})$ as a vector representation of the sentence\n",
        "+ The context vector $(z)$ as an abstract representation of the entire input sentence\n",
        "\n",
        "+ **Decoder**, one per time-step.\n",
        "\n",
        "    In the decoder, we need to go from the hidden state to an actual word, therefore at each time-step we use $s_t$ to predict (by passing it through a `Linear` layer, shown in purple) what we think is the next word in the sequence, $\\hat{y}_t$. \n",
        "\n",
        "    $$\\hat{y}_t = f(s_t)$$\n",
        "\n",
        "    The words in the decoder are always generated one after another, with one per time-step. \n",
        "\n",
        "+ **Teacher Forcing**: ground truth + predicted word    \n",
        "    \n",
        "    We always use `<sos>` for the first input to the decoder, $y_1$, but for subsequent inputs, $y_{t>1}$, we will sometimes use the actual, ground truth next word in the sequence, $y_t$ and sometimes use the word predicted by our decoder, $\\hat{y}_{t-1}$. This is called *teacher forcing*, see a bit more info about it [Teacher Forcing](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/). \n",
        "\n",
        "+ **Know the length in advance**\n",
        "\n",
        "    When training/testing our model, we always know how many words are in our target sentence, so we stop generating words once we hit that many. During inference it is common to keep generating words until the model outputs an `<eos>` token or after a certain amount of words have been generated.\n",
        "\n",
        "+ **Calculate the loss**\n",
        "\n",
        "    Once we have our predicted target sentence, $\\hat{Y} = \\{ \\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_T \\}$, we compare it against our actual target sentence, $Y = \\{ y_1, y_2, ..., y_T \\}$, to calculate our loss. We then use this loss to update all of the parameters in our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTGUe2Ax-QTu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}