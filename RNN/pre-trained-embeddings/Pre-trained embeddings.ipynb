{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained embeddings and NLP corpora\n",
    "Gensim has some really nice functionality, in that it allows you to use pre-trained GloVe and Word2Vec embeddings with its libraries. In addition there are also some re-usable corpora that you can download and immediately use to train a Word2Vec embedding. The code snippets below show you how. The source of the embeddings can be found here: https://github.com/RaRe-Technologies/gensim-data.\n",
    "\n",
    "I'll have to warn you that I'm not impressed with the quality of the pre-trained word embeddings. Either the dataset is noisy or its just too general. To be explained more later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import numpy as np \n",
    "from scipy.linalg import norm\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300',\n",
      " 'conceptnet-numberbatch-17-06-300',\n",
      " 'word2vec-ruscorpora-300',\n",
      " 'word2vec-google-news-300',\n",
      " 'glove-wiki-gigaword-50',\n",
      " 'glove-wiki-gigaword-100',\n",
      " 'glove-wiki-gigaword-200',\n",
      " 'glove-wiki-gigaword-300',\n",
      " 'glove-twitter-25',\n",
      " 'glove-twitter-50',\n",
      " 'glove-twitter-100',\n",
      " 'glove-twitter-200',\n",
      " '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained: Twitter GloVe Embeddings\n",
    "\n",
    "This first step downloads the pre-trained embeddings and loads it for re-use. Note that these are GloVe embeddings built using Tweets as the name suggests. These vectors are based on 2B tweets, 27B tokens, 1.2M vocab, uncased. The original source can be found here: https://nlp.stanford.edu/projects/glove/. The 25 in the model name refers to the dimensionality of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model and return as object ready for use\n",
    "dimension = 25\n",
    "model_glove_twitter = api.load(\"glove-twitter-25\")\n",
    "# model_glove_twitter = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('facebook', 0.948005199432373),\n",
       " ('tweet', 0.9403423070907593),\n",
       " ('fb', 0.9342359900474548),\n",
       " ('instagram', 0.9104822874069214),\n",
       " ('chat', 0.8964964747428894),\n",
       " ('hashtag', 0.8885936737060547),\n",
       " ('tweets', 0.8878158330917358),\n",
       " ('tl', 0.8778460621833801),\n",
       " ('link', 0.877821147441864),\n",
       " ('internet', 0.8753897547721863)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter.most_similar(\"twitter\",topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have loaded the pre-trained model, just use it as you would with any gensim word2vec model. Here are a few similarity examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clegg', 0.9653650522232056),\n",
       " ('miliband', 0.9515050053596497),\n",
       " ('bachmann', 0.9484400749206543),\n",
       " ('mcconnell', 0.9416398406028748),\n",
       " ('carney', 0.9340257048606873),\n",
       " ('coulter', 0.9311323165893555),\n",
       " ('boehner', 0.9286302328109741),\n",
       " ('santorum', 0.9269059896469116),\n",
       " ('farage', 0.9193653464317322),\n",
       " ('mourdock', 0.9186689853668213)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter.most_similar(\"pelosi\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('policy', 0.9484812617301941),\n",
       " ('reforms', 0.9403934478759766),\n",
       " ('laws', 0.9401204586029053),\n",
       " ('government', 0.923071026802063),\n",
       " ('regulations', 0.9168933629989624),\n",
       " ('economy', 0.9110006093978882),\n",
       " ('immigration', 0.9105909466743469),\n",
       " ('legislation', 0.9089650511741638),\n",
       " ('govt', 0.9054747223854065),\n",
       " ('regulation', 0.9050779342651367)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter.most_similar(\"policies\",topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of these words don't fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'orange'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what doesn't fit?\n",
    "model_glove_twitter.doesnt_match([\"trump\",\"bernie\",\"obama\",\"pelosi\",\"orange\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vectors for trump and obama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.56174 ,  0.69419 ,  0.16733 ,  0.055867, -0.26266 , -0.6303  ,\n",
       "       -0.28311 , -0.88244 ,  0.57317 , -0.82376 ,  0.46728 ,  0.48607 ,\n",
       "       -2.1942  , -0.41972 ,  0.31795 , -0.70063 ,  0.060693,  0.45279 ,\n",
       "        0.6564  ,  0.20738 ,  0.84496 , -0.087537, -0.38856 , -0.97028 ,\n",
       "       -0.40427 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show weight vector for trump and obama\n",
    "model_glove_twitter[\"trump\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.77126 ,  0.81259 , -0.5901  , -0.015908, -0.082797, -1.2261  ,\n",
       "        0.098286,  0.087488,  0.012586, -0.35884 ,  0.80733 ,  0.12569 ,\n",
       "       -4.0522  ,  0.14856 ,  0.6988  , -0.78948 , -0.77125 ,  0.49512 ,\n",
       "        0.16366 , -0.9713  ,  0.95064 ,  0.19921 , -0.27903 , -1.6844  ,\n",
       "       -0.79424 ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter['obama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the document similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9069623250399094\n"
     ]
    }
   ],
   "source": [
    "def vector_similarity(s1, s2, dimension = 25):\n",
    "    \n",
    "    def sentence_vector(s):\n",
    "        '''这边dimension取决于你训练模型时，给定的维度\n",
    "        '''\n",
    "        # words = jieba.lcut(s)\n",
    "        words = [ w.lower() for w in s.split()]\n",
    "        \n",
    "        # average the words vector, to get the sentence vector\n",
    "        v = np.zeros(dimension)  \n",
    "        for word in words:\n",
    "            v += model_glove_twitter[word]\n",
    "        v /= len(words)\n",
    "        return v\n",
    "    \n",
    "    v1, v2 = sentence_vector(s1), sentence_vector(s2)\n",
    "    return np.dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "new1 = \"Amazon holds early lead in historic union election\"\n",
    "new2 = \"The woman who took on google and won\"\n",
    "\n",
    "score = vector_similarity(s1=new1, s2=new2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code decomposition explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon', 'holds', 'early', 'lead', 'in', 'historic', 'union', 'election']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = new1\n",
    "words = [ w.lower() for w in s.split()]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension =25\n",
    "v = np.zeros(dimension)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.25367999   2.7157019   -2.38962002  -4.63931996   0.09306997\n",
      "  -4.19992995   5.07026005  -4.96010609   2.51256605  -2.76789112\n",
      "   1.45824202   2.09489102 -28.72370052   7.32691002   1.65678005\n",
      "  -3.59892997  -1.06501414   1.02622299  -1.78564898  -1.97814004\n",
      "  -4.28357     -2.51712359   2.47142602  -5.37796997  -2.65966394]\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    v += model_glove_twitter[word]\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23029 , -0.26417 , -0.19669 , -1.2001  ,  0.84545 , -0.49428 ,\n",
       "        0.46503 , -0.079233,  0.46324 , -0.70849 ,  0.91901 ,  0.65455 ,\n",
       "       -2.73    , -0.74847 , -0.85378 , -0.57711 ,  0.1443  ,  0.33378 ,\n",
       "        0.062339,  0.77928 , -0.77372 , -2.8468  ,  0.22277 , -0.39313 ,\n",
       "       -0.044044], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter[\"amazon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
