{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dataset Reference\n",
    "\n",
    "IMDB的数据\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "https://github.com/SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchtext.vocab as torchvocab\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import tqdm\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import random\n",
    "import snowballstemmer\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from itertools import chain\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义读数的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readIMDB(path, seg='train'):\n",
    "    pos_or_neg = ['pos', 'neg']\n",
    "    data = []\n",
    "    for label in pos_or_neg:\n",
    "        files = os.listdir(os.path.join(path, seg, label))\n",
    "        for file in files:\n",
    "            with open(os.path.join(path, seg, label, file), 'r', encoding='utf8') as rf:\n",
    "                review = rf.read().replace('\\n', '')\n",
    "                if label == 'pos':\n",
    "                    data.append([review, 1])\n",
    "                elif label == 'neg':\n",
    "                    data.append([review, 0])\n",
    "    return data\n",
    "\n",
    "train_data = readIMDB('data/aclImdb')\n",
    "test_data = readIMDB('data/aclImdb', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.',\n",
       " 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Based on an actual story, John Boorman shows the struggle of an American doctor, whose husband and son were murdered and she was continually plagued with her loss. A holiday to Burma with her sister seemed like a good idea to get away from it all, but when her passport was stolen in Rangoon, she could not leave the country with her sister, and was forced to stay back until she could get I.D. papers from the American embassy. To fill in a day before she could fly out, she took a trip into the countryside with a tour guide. \"I tried finding something in those stone statues, but nothing stirred in me. I was stone myself.\" <br /><br />Suddenly all hell broke loose and she was caught in a political revolt. Just when it looked like she had escaped and safely boarded a train, she saw her tour guide get beaten and shot. In a split second she decided to jump from the moving train and try to rescue him, with no thought of herself. Continually her life was in danger. <br /><br />Here is a woman who demonstrated spontaneous, selfless charity, risking her life to save another. Patricia Arquette is beautiful, and not just to look at; she has a beautiful heart. This is an unforgettable story. <br /><br />\"We are taught that suffering is the one promise that life always keeps.\"',\n",
       " 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  comment\n",
       "0      0    12500\n",
       "1      1    12500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_data,columns=[\"comment\",\"label\"])\n",
    "train_df.groupby([\"label\"]).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  comment\n",
       "0      0     1840\n",
       "1      1    12500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(test_data,columns=[\"comment\",\"label\"])\n",
    "test_df.groupby([\"label\"]).count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分詞\n",
    "接着是分词，这里只做非常简单的分词，也就是按照空格分词。当然按照一些传统的清洗方式效果会更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252192 unique words in the data set\n"
     ]
    }
   ],
   "source": [
    "# \"vocab\" would be all the words appear in the train_data_set\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok.lower() for tok in text.split(' ')]\n",
    "\n",
    "train_tokenized = []\n",
    "test_tokenized = []\n",
    "for review, score in train_data:\n",
    "    train_tokenized.append(tokenizer(review))\n",
    "for review, score in test_data:\n",
    "    test_tokenized.append(tokenizer(review))\n",
    "\n",
    "# itertools.chain, Combine N list to form a new single list.\n",
    "vocab = set(chain(*train_tokenized))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"{} unique words in the data set\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word embedding\n",
    "\n",
    "因为这个数据集非常小，所以如果我们用这个数据集做word embedding有可能过拟合，而且模型没有通用性，所以我们传入一个已经学好的word embedding。 用的是glove-twitter-100, 387MB, 100維度的數據。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model and return as object ready for use\n",
    "dimension = 100\n",
    "model_glove_twitter = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44104  ,  0.1385   , -0.66489  , -0.044309 ,  0.44579  ,\n",
       "        0.027886 , -0.30068  , -0.13851  ,  0.44771  ,  0.60006  ,\n",
       "        0.12149  , -0.69262  , -3.5289   , -0.5495   , -0.98539  ,\n",
       "        0.54288  , -0.17355  , -0.73415  , -0.46325  , -0.68942  ,\n",
       "       -0.29029  , -0.20679  , -1.0008   , -0.010779 , -0.52833  ,\n",
       "       -2.9566   ,  0.45207  , -0.65441  ,  0.10636  ,  0.15182  ,\n",
       "       -0.71115  ,  0.17282  , -0.16225  , -0.96776  ,  0.64226  ,\n",
       "       -0.029472 ,  0.5799   ,  0.18865  , -0.022253 , -0.61489  ,\n",
       "       -1.1467   ,  0.39476  , -0.2715   , -0.024786 ,  0.32542  ,\n",
       "       -0.14626  , -0.13835  ,  0.44469  , -0.72034  ,  0.0059288,\n",
       "        0.069213 , -0.042943 , -0.32557  , -0.4062   , -0.023224 ,\n",
       "        0.74154  , -1.5501   , -0.012535 , -0.020187 , -0.31557  ,\n",
       "        0.036324 , -0.56278  ,  0.072553 , -0.02491  , -0.53492  ,\n",
       "        0.49579  ,  0.24916  ,  0.92282  , -0.20315  ,  0.27591  ,\n",
       "       -0.71818  ,  0.39903  , -0.078875 , -0.38303  , -0.84732  ,\n",
       "        0.80215  , -0.59038  , -0.30123  ,  0.034802 ,  0.34928  ,\n",
       "        0.36331  ,  0.2053   ,  0.44075  , -0.15293  , -0.16563  ,\n",
       "       -0.21373  , -0.3372   ,  0.10873  ,  0.23909  , -0.68149  ,\n",
       "       -0.33458  , -0.2037   , -0.14106  , -0.1777   ,  0.58009  ,\n",
       "       -0.38089  ,  0.20533  ,  0.17301  ,  0.079456 , -0.25333  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter[\"twitter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义一个word to index的词典 \n",
    "\n",
    "定义的目的是为了将预训练的weight跟我们的词库拼上。    \n",
    "另外我们定义了一个unknown的词，也就是说没有出现在训练集里的词，我们都叫做unknown，词向量就定义为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unknow word index=0, the other word start from index=1\n",
    "word_to_idx = {word: i+1 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<unk>'] = 0\n",
    "\n",
    "idx_to_word = {i+1: word for i, word in enumerate(vocab)}\n",
    "idx_to_word[0] = '<unk>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码\n",
    "\n",
    "我们这里为了解决评论长度不一致的问题，将所有的评论都取500个词，超过的就取前500个，不足的补0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_samples(tokenized_samples, vocab):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in word_to_idx:\n",
    "                feature.append(word_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def pad_samples(features, maxlen=500, PAD=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) >= maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            while(len(padded_feature) < maxlen):\n",
    "                padded_feature.append(PAD)\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"vocab\" would be all the words appear in the train_data_set\n",
    "\n",
    "# \"train_tokenized\" would be [[tokenized word in comment1],[tokenized word in comment2],...] \n",
    "# (i.g. train_tokenized[0:2])\n",
    "\n",
    "# \"train_data\" would be [[comment1,label1],[comment2,label2]...]\n",
    "# (i.g. train_data[0:2])\n",
    "\n",
    "train_features = torch.tensor(pad_samples(encode_samples(train_tokenized, vocab)))\n",
    "train_labels = torch.tensor([score for _, score in train_data])\n",
    "test_features = torch.tensor(pad_samples(encode_samples(test_tokenized, vocab)))\n",
    "test_labels = torch.tensor([score for _, score in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features = torch.Size([25000, 500])\n",
      "train_labels = torch.Size([25000])\n",
      "test_features = torch.Size([14340, 500])\n",
      "test_labels = torch.Size([14340])\n"
     ]
    }
   ],
   "source": [
    "print(\"train_features = {}\".format(train_features.shape))\n",
    "print(\"train_labels = {}\".format(train_labels.shape))\n",
    "print(\"test_features = {}\".format(test_features.shape))\n",
    "print(\"test_labels = {}\".format(test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "\n",
    "# create the pretrained - word Embedding\n",
    "weight = torch.zeros(vocab_size+1, embed_size)\n",
    "\n",
    "for i in range(len(model_glove_twitter.index_to_key)):\n",
    "    try:\n",
    "        # use this statement to capture the case that glove_word not in our word_base\n",
    "        # if glove_word is in word_to_idx, get the word index\n",
    "        # else raise Error\n",
    "        glove_word = model_glove_twitter.index_to_key[i]\n",
    "        index = word_to_idx[glove_word]\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    glove_word = model_glove_twitter.index_to_key[i]\n",
    "    index = word_to_idx[glove_word]\n",
    "    word = idx_to_word[index]\n",
    "    weight[index, :] = torch.from_numpy(model_glove_twitter.get_vector(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-train model have 1193514 key words\n"
     ]
    }
   ],
   "source": [
    "print(\"pre-train model have {} key words\".format( len(model_glove_twitter.index_to_key) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twitter'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter.index_to_key[218]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter.key_to_index[\"twitter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193528"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx['twitter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_word = school\n",
      "index = 180839\n",
      "word = school\n",
      "output vector = (100,)\n",
      "[ 0.11078   -0.24165    0.58407    0.33004   -0.28523    0.39694\n",
      "  0.48903    1.3905    -0.75806   -0.0066963 -0.30907    0.18059\n",
      " -4.5539     0.46777   -0.35242   -0.28211   -0.81762   -0.16247\n",
      " -1.4519    -0.54034   -0.20429    0.048978  -0.67599    0.56651\n",
      "  0.66277   -0.094981  -0.39577   -0.88254    0.007897  -1.0843\n",
      " -0.35871   -0.21915   -0.26527   -0.032049   0.02007    0.44382\n",
      "  0.39047    1.2238     0.62872   -0.1833    -1.1305     0.29216\n",
      " -0.15579   -0.357     -0.071372   0.15666    0.11832    0.44056\n",
      " -0.9165    -0.17102    0.33795    0.14644   -0.4715     0.22617\n",
      " -0.53264   -0.43504   -0.73209   -0.23924   -0.78109    0.37778\n",
      " -0.5164     0.030757   0.49751   -0.79576   -0.63072   -0.40766\n",
      "  0.032427  -0.31238    0.54056   -0.99269   -0.17894    0.096784\n",
      "  0.34071    0.27062   -0.23308   -0.050616   0.59956   -0.29787\n",
      "  0.4078    -0.50386    1.7127    -0.72348    0.30162   -0.38823\n",
      "  0.53881   -0.20311   -0.11727   -0.34253    0.50525   -0.32589\n",
      "  0.14619    0.84863    0.32034    0.52919   -0.36471   -0.42496\n",
      "  0.14915    0.4148     0.25441    0.37878  ]\n",
      "pretrained-embedding(weight.shape) = torch.Size([252193, 100])\n"
     ]
    }
   ],
   "source": [
    "i = 281\n",
    "glove_word = model_glove_twitter.index_to_key[i]\n",
    "print(f\"glove_word = {glove_word}\")\n",
    "index = word_to_idx[glove_word]\n",
    "print(f\"index = {index}\")\n",
    "word = idx_to_word[index]\n",
    "print(f\"word = {word}\")\n",
    "vector = model_glove_twitter.get_vector(word)\n",
    "print(f\"output vector = {vector.shape}\")\n",
    "print(vector)\n",
    "\n",
    "print(f\"pretrained-embedding(weight.shape) = {weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, seq_len, labels, weight, **kwargs):\n",
    "        super(textCNN, self).__init__(**kwargs)\n",
    "        self.labels = labels\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.conv1 = nn.Conv2d(1, 1, (3, embed_size))\n",
    "        self.conv2 = nn.Conv2d(1, 1, (4, embed_size))\n",
    "        self.conv3 = nn.Conv2d(1, 1, (5, embed_size))\n",
    "        self.pool1 = nn.MaxPool2d((seq_len - 3 + 1, 1))\n",
    "        self.pool2 = nn.MaxPool2d((seq_len - 4 + 1, 1))\n",
    "        self.pool3 = nn.MaxPool2d((seq_len - 5 + 1, 1))\n",
    "        self.linear = nn.Linear(3, labels)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs).view(inputs.shape[0], 1, inputs.shape[1], -1)\n",
    "        x1 = F.relu(self.conv1(inputs))\n",
    "        x2 = F.relu(self.conv2(inputs))\n",
    "        x3 = F.relu(self.conv3(inputs))\n",
    "        \n",
    "        x1 = self.dropout(x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        x3 = self.dropout(x3)\n",
    "\n",
    "        x1 = self.pool1(x1)\n",
    "        x2 = self.pool2(x2)\n",
    "        x3 = self.pool3(x3)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), -1)\n",
    "        x = x.view(inputs.shape[0], 1, -1)\n",
    "\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1, self.labels)\n",
    "\n",
    "        return(x)\n",
    "    \n",
    "num_epochs = 5\n",
    "num_hiddens = 100\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 64\n",
    "labels = 2\n",
    "lr = 0.8\n",
    "device = torch.device('cuda:0')\n",
    "use_gpu = False\n",
    "\n",
    "\n",
    "net = textCNN(vocab_size=(vocab_size+1), embed_size=embed_size,\n",
    "              seq_len=500, labels=labels, weight=weight)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textCNN(\n",
       "  (embedding): Embedding(252193, 100)\n",
       "  (conv1): Conv2d(1, 1, kernel_size=(3, 100), stride=(1, 1))\n",
       "  (conv2): Conv2d(1, 1, kernel_size=(4, 100), stride=(1, 1))\n",
       "  (conv3): Conv2d(1, 1, kernel_size=(5, 100), stride=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=(498, 1), stride=(498, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool2): MaxPool2d(kernel_size=(497, 1), stride=(497, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool3): MaxPool2d(kernel_size=(496, 1), stride=(496, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear): Linear(in_features=3, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_set = torch.utils.data.TensorDataset(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 0.6972, train acc: 0.50, test loss: 0.6410, test acc: 0.87, time: 163.79\n",
      "epoch: 1, train loss: 0.6938, train acc: 0.51, test loss: 0.7891, test acc: 0.15, time: 162.95\n",
      "epoch: 2, train loss: 0.6920, train acc: 0.52, test loss: 0.7723, test acc: 0.19, time: 162.81\n",
      "epoch: 3, train loss: 0.6904, train acc: 0.52, test loss: 0.7834, test acc: 0.29, time: 162.44\n",
      "epoch: 4, train loss: 0.6866, train acc: 0.54, test loss: 0.6926, test acc: 0.45, time: 163.34\n"
     ]
    }
   ],
   "source": [
    "train_loss_lst = []\n",
    "train_acc_lst = []\n",
    "test_loss_lst = []\n",
    "test_acc_lst = [] \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss, test_losses = 0, 0\n",
    "    train_acc, test_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "    for feature, label in train_iter:\n",
    "        n += 1\n",
    "        net.train()\n",
    "        net.zero_grad()\n",
    "        feature = Variable(feature)\n",
    "        label = Variable(label)\n",
    "        score = net(feature)\n",
    "        loss = loss_function(score, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                 dim=1), label.cpu())\n",
    "        train_loss += loss\n",
    "    with torch.no_grad():\n",
    "        for test_feature, test_label in test_iter:\n",
    "            m += 1\n",
    "            net.eval()\n",
    "            test_feature = test_feature\n",
    "            test_label = test_label\n",
    "            test_score = net(test_feature)\n",
    "            test_loss = loss_function(test_score, test_label)\n",
    "            test_acc += accuracy_score(torch.argmax(test_score.cpu().data,\n",
    "                                                    dim=1), test_label.cpu())\n",
    "            test_losses += test_loss\n",
    "    end = time.time()\n",
    "    runtime = end - start\n",
    "    print('epoch: %d, train loss: %.4f, train acc: %.2f, test loss: %.4f, test acc: %.2f, time: %.2f' %\n",
    "          (epoch, train_loss.data / n, train_acc / n, test_losses.data / m, test_acc / m, runtime))\n",
    "\n",
    "    train_loss_lst.append(train_loss.data / n)\n",
    "    train_acc_lst.append(train_acc / n)\n",
    "    test_loss_lst.append(test_losses.data / m)\n",
    "    test_acc_lst.append(test_acc / m)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     net.eval()\n",
    "#     for test_feature, test_label in test_iter:\n",
    "#         test_feature = test_feature\n",
    "#         test_label = test_label\n",
    "#         test_score = net(test_feature)\n",
    "#         test_loss = loss_function(test_score, test_label)\n",
    "#         break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
